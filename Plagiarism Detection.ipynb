{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram similarity of documents using ferret technique is:  0.7425742574257426\n",
      "Trigram similarity of documents using containment technique is: 0.8754863813229572\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Different plagiarism examples can be tried from: https://www.bowdoin.edu/dean-of-students/judicial-board/academic-honesty-and-plagiarism/examples.html#Direct\n",
    "original_text = '''\n",
    "                In ages which have no record these islands were the home of millions of happy birds, the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                '''\n",
    "suspicious_text = '''\n",
    "                 Long ago, when there was no written history, these islands were the home of millions of happy birds; the resort of a hundred times more millions of fishes, sea lions, and other creatures. Here lived innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                 '''\n",
    "\"\"\"\n",
    "This section contains text preprocessing functions later required by plagiarism detection technique\n",
    "\"\"\"\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized\n",
    "\n",
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        word_tokenized.append(word_tokenize(sentence))\n",
    "    return ','.join([str(x) for x in word_tokenized])\n",
    "    \n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "    \n",
    "#print(text_tokenization(text_segmentation(suspicious_text)))\n",
    "\n",
    "\n",
    "'''\n",
    "Following block represents all the functionality corresponding to trigram similarity technique\n",
    "'''\n",
    "\n",
    "'''\n",
    "Utility Functions\n",
    "'''\n",
    "\n",
    "'''\n",
    "This function generates ngrams from a tokenized/non-tokenized text\n",
    "It accepts text, ngram(n=3 for trigram), and boolean tokenized as parameter\n",
    "'''\n",
    "def generate_ngrams(text,n,tokenized=True):\n",
    "    trigram_arr = []\n",
    "    if tokenized == False:\n",
    "        text = text.split() #Basic tokenization of text if not already tokenized\n",
    "    trigrams =  ngrams(text, 3)\n",
    "    for gram in trigrams:\n",
    "        #print(grams)\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr\n",
    "\n",
    "'''\n",
    "This function compares two sets to generate their jaccard similarity score.\n",
    "It accepts two sets and a technique(Ferret or Containment) as parameter.\n",
    "'''\n",
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    " intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    " union_cardinality = 1\n",
    " if technique == \"Ferret\":\n",
    "     union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    " else:\n",
    "     union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    " return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "\n",
    "#Preprocessing for trigram similarity method\n",
    "original_text = text_tokenization(text_segmentation(text_lowercase(original_text))) #Apply lowercasing,Segmentation,Tokenization as mention in research paper\n",
    "suspicious_text = text_tokenization(text_segmentation(text_lowercase(suspicious_text))) #Apply lowercasing,Segmentation,Tokenization as mention in research paper\n",
    "\n",
    "#trigram generation for both documents\n",
    "n = 3\n",
    "trigrams_original_text =  generate_ngrams(original_text,n,True)\n",
    "trigrams_suspicious_text =  generate_ngrams(suspicious_text,n,True)\n",
    "\n",
    "#Trigram Similarity Calculation\n",
    "ferret_trigram_similarity = jaccard_similarity(trigrams_original_text,trigrams_suspicious_text,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(trigrams_original_text,trigrams_suspicious_text,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
